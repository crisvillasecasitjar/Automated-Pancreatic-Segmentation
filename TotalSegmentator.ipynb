{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-10 09:32:05.038104: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-09-10 09:32:05.095065: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-09-10 09:32:05.920033: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import nibabel as nib\n",
    "import numpy as np\n",
    "from totalsegmentator.python_api import totalsegmentator\n",
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "import nibabel as nib\n",
    "from monai.transforms import (\n",
    "    Compose, LoadImaged, EnsureChannelFirstd, \n",
    "    ThresholdIntensityd, NormalizeIntensityd, Spacingd, Lambda, AsDiscreted\n",
    ")\n",
    "from monai.data import Dataset, DataLoader\n",
    "from monai.networks.nets import SwinUNETR\n",
    "from monai.metrics import DiceMetric\n",
    "from monai.losses import DiceLoss\n",
    "from monai.transforms import AsDiscreted\n",
    "import numpy as np\n",
    "from tqdm import trange\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths and device configuration\n",
    "data_dir = \"/data/PANORAMA/cvillaseca/NETWORKS_SEGMENTACIO/BASELINE_SEGMENTACIO/data\"  \n",
    "csv_file = \"/data/PANORAMA/cvillaseca/NETWORKS_SEGMENTACIO/BASELINE_SEGMENTACIO/data/infer.csv\"  \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#device= torch.device('cpu')\n",
    "\n",
    "# Load the CSV file into a pandas DataFrame\n",
    "df = pd.read_csv(csv_file)\n",
    "\n",
    "# Define test transforms\n",
    "wl, ww = -40, 400  # For abdominal window\n",
    "clamp1 = ThresholdIntensityd(keys=('image',), above=False, threshold=wl + (ww / 2), cval=wl + (ww / 2))\n",
    "clamp2 = ThresholdIntensityd(keys=('image',), above=True, threshold=wl - (ww / 2), cval=wl - (ww / 2))\n",
    "norm = NormalizeIntensityd(keys=('image',), nonzero=True)\n",
    "space = Spacingd(keys=('image', 'label'), pixdim=(1.5, 1.5, 1.5), mode=('bilinear', 'nearest'))\n",
    "\n",
    "def correct_label(l):\n",
    "    # https://grand-challenge.org/forums/forum/panorama-pancreatic-cancer-diagnosis-radiologists-meet-ai-711/topic/label-problem-2275/\n",
    "    l[l == 1] = 0\n",
    "    l[l==2] = 0 \n",
    "    l[l==3] = 0 \n",
    "    l[l==4] = 1\n",
    "    l[l==5] = 0 \n",
    "    l[l==6] = 0 \n",
    "    l[l==7] = 0 \n",
    "    l[l==8] = 0 \n",
    "    return l\n",
    "\n",
    "test_org_transforms = Compose([\n",
    "    LoadImaged(keys=[\"image\", \"label\"]),\n",
    "    EnsureChannelFirstd(keys=[\"image\", \"label\"]),\n",
    "    clamp1,\n",
    "    clamp2,\n",
    "    norm,\n",
    "    space,\n",
    "    Lambda(lambda d: {'image': d['image'], 'label': correct_label(d['label'])}),\n",
    "    AsDiscreted(keys=('label'), to_onehot=2)\n",
    "\n",
    "])\n",
    "\n",
    "# Create a MONAI dataset from the CSV file\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, df, transforms=None):\n",
    "        self.df = df\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        data_dict = {\n",
    "            'image': self.df.iloc[index]['scan'],\n",
    "            'label': self.df.iloc[index]['label']\n",
    "        }\n",
    "        if self.transforms:\n",
    "            data_dict = self.transforms(data_dict)\n",
    "        return data_dict\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "# Initialize the dataset and data loader\n",
    "test_dataset = CustomDataset(df=df, transforms=test_org_transforms)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fast_multiclass_dice(actual, predicted, n_classes):\n",
    "    actual = np.squeeze(np.array(actual))\n",
    "    predicted = np.squeeze(np.array(predicted))\n",
    "    print('actual shape:', actual.shape)\n",
    "    print('actual dtype', actual.dtype)\n",
    "    print('predicted shape:', predicted.shape)\n",
    "    print('predicted dtype', predicted.dtype)\n",
    "\n",
    "    # Initialize an array to store the dice score for each class\n",
    "    dices = np.zeros(n_classes) \n",
    "    for cls in range(n_classes):\n",
    "        actual_cls = (actual == cls)\n",
    "        predicted_cls = (predicted == cls)\n",
    "        actual_cls = np.array(actual_cls).astype(bool)\n",
    "        predicted_cls = np.array(predicted_cls).astype(bool)\n",
    "        print('actual_cls shape:', actual_cls.shape)\n",
    "        print('actual_cls dtype', actual_cls.dtype)\n",
    "        print('predicted_cls shape:', predicted_cls.shape)\n",
    "        print('predicted_cls dtype', predicted_cls.dtype)\n",
    "        \n",
    "        intersections = np.logical_and(actual_cls, predicted_cls).sum(axis=(0, 1, 2))\n",
    "        im_sums = actual_cls.sum(axis=(0, 1, 2)) + predicted_cls.sum(axis=(0, 1, 2))\n",
    "        dices[cls] = 2. * intersections / np.maximum(im_sums, 1e-6)\n",
    "    return dices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define post-processing transforms to get predictions as discrete values\n",
    "post_transform = Compose([\n",
    "    AsDiscreted(keys=\"pred\", argmax=True),  # Get class indices from the 5-class predictions\n",
    "    Lambda(lambda d: {'pred': map_classes_to_binary(d['pred']), 'label': d['label']}),  # Map class 4 to foreground, rest to background\n",
    "])\n",
    "\n",
    "def map_classes_to_binary(pred):\n",
    "    pred = pred.long()\n",
    "\n",
    "    print(f\"Raw prediction shape: {pred.shape}, unique values: {torch.unique(pred)}\")\n",
    "\n",
    "    # Initialize all as background (0)\n",
    "    binary_pred = torch.zeros_like(pred)\n",
    "\n",
    "    # Map class 4 to foreground (1)\n",
    "    binary_pred[pred == 4] = 1\n",
    "\n",
    "    print(f\"Binary prediction shape: {binary_pred.shape}, unique values: {torch.unique(binary_pred)}\")\n",
    "\n",
    "    return binary_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = \"/data/PANORAMA/cvillaseca/NETWORKS_SEGMENTACIO/BASELINE_SEGMENTACIO/test_results/Total_segmentator\"\n",
    "log_file = os.path.join(log_dir, \"dsc_log_TS.txt\")\n",
    "# Ensure the directory exists\n",
    "if not os.path.exists(log_dir):\n",
    "    os.makedirs(log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/ubuntu/.totalsegmentator/nnunet/results/Dataset298_TotalSegmentator_total_6mm_1559subj/nnUNetTrainer_4000epochs_NoMirroring__nnUNetPlans__3d_fullres/plans.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m nifti_img \u001b[38;5;241m=\u001b[39m nib\u001b[38;5;241m.\u001b[39mNifti1Image(np\u001b[38;5;241m.\u001b[39msqueeze(i[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimage\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy()), affine\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39meye(\u001b[38;5;241m4\u001b[39m))\n\u001b[1;32m      5\u001b[0m roi_subset \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mliver\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mkidney_left\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mkidney_right\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpancreas\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspleen\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m----> 6\u001b[0m prediction_ts \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mtranspose(np\u001b[38;5;241m.\u001b[39marray(\u001b[43mtotalsegmentator\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnifti_img\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/data/PANORAMA/cvillaseca/NETWORKS_SEGMENTACIO/BASELINE_SEGMENTACIO/test_results/Total_segmentator\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mroi_subset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mroi_subset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquiet\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtotal\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mskip_saving\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mget_fdata()), (\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m1\u001b[39m))\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39muint8)\n\u001b[1;32m      7\u001b[0m output_nifti \u001b[38;5;241m=\u001b[39m nib\u001b[38;5;241m.\u001b[39mNifti1Image(prediction_ts, affine\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39meye(\u001b[38;5;241m4\u001b[39m))\n\u001b[1;32m      8\u001b[0m output_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/data/PANORAMA/cvillaseca/NETWORKS_SEGMENTACIO/BASELINE_SEGMENTACIO/test_results/Total_segmentator\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.10/site-packages/totalsegmentator/python_api.py:346\u001b[0m, in \u001b[0;36mtotalsegmentator\u001b[0;34m(input, output, ml, nr_thr_resamp, nr_thr_saving, fast, nora_tag, preview, task, roi_subset, statistics, radiomics, crop_path, body_seg, force_split, output_type, quiet, verbose, test, skip_saving, device, license_number, statistics_exclude_masks_at_border, no_derived_masks, v1_order, fastest, roi_subset_robust)\u001b[0m\n\u001b[1;32m    343\u001b[0m crop_trainer \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnnUNetTrainer_DASegOrd0_NoMirroring\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m task \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtotal_mr\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnnUNetTrainer_4000epochs_NoMirroring\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    344\u001b[0m download_pretrained_weights(crop_model_task)\n\u001b[0;32m--> 346\u001b[0m organ_seg, _, _ \u001b[38;5;241m=\u001b[39m \u001b[43mnnUNet_predict_image\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcrop_model_task\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m3d_fullres\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfolds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    347\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcrop_trainer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtta\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmultilabel_image\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresample\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcrop_spacing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mcrop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcrop_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtask_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcrop_task\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnora_tag\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mNone\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreview\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m                    \u001b[49m\u001b[43msave_binary\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnr_threads_resampling\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnr_thr_resamp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnr_threads_saving\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mcrop_addon\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcrop_addon\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstatistics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mquiet\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquiet\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskip_saving\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    352\u001b[0m class_map_inv \u001b[38;5;241m=\u001b[39m {v: k \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m class_map[crop_task]\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[1;32m    353\u001b[0m crop_mask \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros(organ_seg\u001b[38;5;241m.\u001b[39mshape, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39muint8)\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.10/site-packages/totalsegmentator/nnunet.py:471\u001b[0m, in \u001b[0;36mnnUNet_predict_image\u001b[0;34m(file_in, file_out, task_id, model, folds, trainer, tta, multilabel_image, resample, crop, crop_path, task_name, nora_tag, preview, save_binary, nr_threads_resampling, nr_threads_saving, force_split, crop_addon, roi_subset, output_type, statistics, quiet, verbose, test, skip_saving, device, exclude_masks_at_border, no_derived_masks, v1_order)\u001b[0m\n\u001b[1;32m    467\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m test \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    468\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m nostdout(verbose):\n\u001b[1;32m    469\u001b[0m         \u001b[38;5;66;03m# nnUNet_predict(tmp_dir, tmp_dir, task_id, model, folds, trainer, tta,\u001b[39;00m\n\u001b[1;32m    470\u001b[0m         \u001b[38;5;66;03m#                nr_threads_resampling, nr_threads_saving)\u001b[39;00m\n\u001b[0;32m--> 471\u001b[0m         \u001b[43mnnUNetv2_predict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtmp_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtmp_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtask_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfolds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtta\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    472\u001b[0m \u001b[43m                         \u001b[49m\u001b[43mnr_threads_resampling\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnr_threads_saving\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    473\u001b[0m \u001b[43m                         \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquiet\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquiet\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstep_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstep_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    474\u001b[0m \u001b[38;5;66;03m# elif test == 2:\u001b[39;00m\n\u001b[1;32m    475\u001b[0m \u001b[38;5;66;03m#     print(\"WARNING: Using reference seg instead of prediction for testing.\")\u001b[39;00m\n\u001b[1;32m    476\u001b[0m \u001b[38;5;66;03m#     shutil.copy(Path(\"tests\") / \"reference_files\" / \"example_seg_fast.nii.gz\", tmp_dir / f\"s01.nii.gz\")\u001b[39;00m\n\u001b[1;32m    477\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m test \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m3\u001b[39m:\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.10/site-packages/totalsegmentator/nnunet.py:242\u001b[0m, in \u001b[0;36mnnUNetv2_predict\u001b[0;34m(dir_in, dir_out, task_id, model, folds, trainer, tta, num_threads_preprocessing, num_threads_nifti_save, plans, device, quiet, step_size)\u001b[0m\n\u001b[1;32m    230\u001b[0m \u001b[38;5;66;03m# nnUNet >= 2.2.2\u001b[39;00m\n\u001b[1;32m    231\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    232\u001b[0m     predictor \u001b[38;5;241m=\u001b[39m nnUNetPredictor(\n\u001b[1;32m    233\u001b[0m         tile_step_size\u001b[38;5;241m=\u001b[39mstep_size,\n\u001b[1;32m    234\u001b[0m         use_gaussian\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    240\u001b[0m         allow_tqdm\u001b[38;5;241m=\u001b[39mallow_tqdm\n\u001b[1;32m    241\u001b[0m     )\n\u001b[0;32m--> 242\u001b[0m \u001b[43mpredictor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minitialize_from_trained_model_folder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    243\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    244\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_folds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfolds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    245\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcheckpoint_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchk\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    246\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    247\u001b[0m predictor\u001b[38;5;241m.\u001b[39mpredict_from_files(dir_in, dir_out,\n\u001b[1;32m    248\u001b[0m                              save_probabilities\u001b[38;5;241m=\u001b[39msave_probabilities, overwrite\u001b[38;5;241m=\u001b[39m\u001b[38;5;129;01mnot\u001b[39;00m continue_prediction,\n\u001b[1;32m    249\u001b[0m                              num_processes_preprocessing\u001b[38;5;241m=\u001b[39mnpp, num_processes_segmentation_export\u001b[38;5;241m=\u001b[39mnps,\n\u001b[1;32m    250\u001b[0m                              folder_with_segs_from_prev_stage\u001b[38;5;241m=\u001b[39mprev_stage_predictions,\n\u001b[1;32m    251\u001b[0m                              num_parts\u001b[38;5;241m=\u001b[39mnum_parts, part_id\u001b[38;5;241m=\u001b[39mpart_id)\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.10/site-packages/nnunetv2/inference/predict_from_raw_data.py:75\u001b[0m, in \u001b[0;36mnnUNetPredictor.initialize_from_trained_model_folder\u001b[0;34m(self, model_training_output_dir, use_folds, checkpoint_name)\u001b[0m\n\u001b[1;32m     72\u001b[0m     use_folds \u001b[38;5;241m=\u001b[39m nnUNetPredictor\u001b[38;5;241m.\u001b[39mauto_detect_available_folds(model_training_output_dir, checkpoint_name)\n\u001b[1;32m     74\u001b[0m dataset_json \u001b[38;5;241m=\u001b[39m load_json(join(model_training_output_dir, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdataset.json\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m---> 75\u001b[0m plans \u001b[38;5;241m=\u001b[39m \u001b[43mload_json\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_training_output_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mplans.json\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     76\u001b[0m plans_manager \u001b[38;5;241m=\u001b[39m PlansManager(plans)\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(use_folds, \u001b[38;5;28mstr\u001b[39m):\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.10/site-packages/batchgenerators/utilities/file_and_folder_operations.py:68\u001b[0m, in \u001b[0;36mload_json\u001b[0;34m(file)\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_json\u001b[39m(file: \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m---> 68\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m     69\u001b[0m         a \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mload(f)\n\u001b[1;32m     70\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m a\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/ubuntu/.totalsegmentator/nnunet/results/Dataset298_TotalSegmentator_total_6mm_1559subj/nnUNetTrainer_4000epochs_NoMirroring__nnUNetPlans__3d_fullres/plans.json'"
     ]
    }
   ],
   "source": [
    "all_dsc= []\n",
    "for i in test_loader:\n",
    "    test_images, test_labels = i['image'].to(device), i['label']\n",
    "    nifti_img = nib.Nifti1Image(np.squeeze(i['image'].cpu().detach().numpy()), affine=np.eye(4))\n",
    "    roi_subset = ['liver', 'kidney_left', 'kidney_right', 'pancreas', 'spleen']\n",
    "    prediction_ts = np.transpose(np.array(totalsegmentator(input=nifti_img, output='/data/PANORAMA/cvillaseca/NETWORKS_SEGMENTACIO/BASELINE_SEGMENTACIO/test_results/Total_segmentator', roi_subset=roi_subset, quiet=True,skip_saving=False).get_fdata()), (2,0,1)).astype(np.uint8)\n",
    "    output_nifti = nib.Nifti1Image(prediction_ts, affine=np.eye(4))\n",
    "    output_path='/data/PANORAMA/cvillaseca/NETWORKS_SEGMENTACIO/BASELINE_SEGMENTACIO/test_results/Total_segmentator'\n",
    "    nib.save(output_nifti, f'{output_path}/segmentation_result_{i[\"id\"]}.nii.gz')#prediction_ts[prediction_ts != 4] = 0  \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'download_weights' from 'totalsegmentator.python_api' (/opt/conda/envs/pytorch/lib/python3.10/site-packages/totalsegmentator/python_api.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnibabel\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnib\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtotalsegmentator\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython_api\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m totalsegmentator\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtotalsegmentator\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython_api\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m download_weights\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Download the weights\u001b[39;00m\n\u001b[1;32m      6\u001b[0m download_weights()\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'download_weights' from 'totalsegmentator.python_api' (/opt/conda/envs/pytorch/lib/python3.10/site-packages/totalsegmentator/python_api.py)"
     ]
    }
   ],
   "source": [
    "import nibabel as nib\n",
    "from totalsegmentator.python_api import totalsegmentator\n",
    "from totalsegmentator.python_api import download_weights\n",
    "\n",
    "# Download the weights\n",
    "download_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # option 1: provide input and output as file paths\n",
    "    input_path= '/data/PANORAMA/cvillaseca/panorama_batch1/batch_1/100000_00001_0000.nii.gz'\n",
    "    output_path= '/data/PANORAMA/cvillaseca/NETWORKS_SEGMENTACIO/BASELINE_SEGMENTACIO/test_results/Total_segmentator'\n",
    "    totalsegmentator(input_path, output_path)\n",
    "    \n",
    "    # option 2: provide input and output as nifti image objects\n",
    "    input_img = nib.load(input_path)\n",
    "    output_img = totalsegmentator(input_img)\n",
    "    nib.save(output_img, output_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
